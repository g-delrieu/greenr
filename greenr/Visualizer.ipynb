{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_calculation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, df, b, c = calculate('https://www.bbc.co.uk/food/recipes/honeyandmarmaladegla_93186')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = get_ingredients_url('https://www.bbc.co.uk/food/recipes/honeyandmarmaladegla_93186')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,e,f = get_ingredients_url('https://www.bbc.co.uk/food/recipes/loaded_aubergine_burgers_44288')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TEST\n",
    "h,i,j = get_ingredients_url('https://www.bbc.co.uk/food/recipes/great_british_31279')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_summary(summary):\n",
    "\n",
    "    # Remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        summary = str(summary).replace(punctuation, '')\n",
    "\n",
    "    # Lower text\n",
    "    summary = summary.lower()\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    summary_tokenized = word_tokenize(summary)\n",
    "    text = [w for w in summary_tokenized if not w in stop_words]\n",
    "    summary = ' '.join(text)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pre_process_summary(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pre_process_summary(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = pre_process_summary(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_and_score(summary_vector):\n",
    "\n",
    "    scoreseries = []\n",
    "\n",
    "    for j, _ in enumerate(category_summaries):\n",
    "\n",
    "        cosine_sum = 1 - spatial.distance.cosine(\n",
    "            summary_vector.toarray(), category_summary_vectors[j, :].toarray())\n",
    "\n",
    "        scoreseries.append(cosine_sum)\n",
    "\n",
    "    matchscore = max(scoreseries)\n",
    "    match = category_list[scoreseries.index(matchscore)]\n",
    "\n",
    "    return match, matchscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - spatial.distance.cosine(a2.toarray(), h2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matching_objects2.pk', 'rb') as handle:\n",
    "    matching_objects_dict = cPickle.load(handle)\n",
    "    vectorizer = matching_objects_dict['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = vectorizer.transform([a1])\n",
    "d2 = vectorizer.transform([d1])\n",
    "h2 = vectorizer.transform([h1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - spatial.distance.cosine(d2.toarray(), h2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k,l,m = get_ingredients_url('https://www.bbc.co.uk/food/recipes/spinach_and_mushroom_31250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = pre_process_summary(k)\n",
    "k2 = vectorizer.transform([k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - spatial.distance.cosine(d2.toarray(), k2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recipe_ingredients(ingredient_list):\n",
    "\n",
    "    \"\"\"Wrapper around parse_ingredient so we can call it on an ingredient list\"\"\"\n",
    "    sentences = tokenizer.tokenize(ingredient_list)\n",
    "    sentences = [sent.strip('\\n') for sent in sentences]\n",
    "    names = []\n",
    "    qtys = {}\n",
    "    units = {}\n",
    "    our_punctuation = '!\"#$%&\\'())*+:;<=>?@[\\\\]^_`{|}~'\n",
    "    our_units = ['gram', 'milliliters', 'thigh', 'wing', 'breast']\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for punctuation in our_punctuation:\n",
    "        # cleaning for common issues\n",
    "            sentences[i] = sentences[i].replace(punctuation, '')\n",
    "\n",
    "        sentences[i] = sentences[i].replace('can', '')\n",
    "        sentences[i] = sentences[i].replace('package', '')\n",
    "        sentences[i] = sentences[i].replace('container', '')\n",
    "        sentences[i] = sentences[i].replace('eggs eggs', 'eggs')\n",
    "        sentences[i] = sentences[i].replace('⅓', '.33')\n",
    "        sentences[i] = sentences[i].replace('½', '.5')\n",
    "        sentences[i] = sentences[i].replace('¼', '.25')\n",
    "        sentences[i] = sentences[i].replace('¾', '.75')\n",
    "        sentences[i] = sentences[i].replace('tsp', 'teaspoon')\n",
    "        sentences[i] = sentences[i].replace('tbsp', 'tablespoon')\n",
    "        sentences[i] = sentences[i].replace('large', '')\n",
    "        sentences[i] = sentences[i].replace('medium', '')\n",
    "        sentences[i] = sentences[i].replace('small', '')\n",
    "        sentences[i] = sentences[i].replace('kg', '000g')\n",
    "        sentences[i] = sentences[i].replace('aubergine', 'eggplant')\n",
    "        sentences[i] = sentences[i].replace('free-range', '')\n",
    "\n",
    "        if re.search(\"\\dg\", sentences[i]) is not None:\n",
    "            sentences[i] = sentences[i].replace(\"g\", \"gram\", 1)\n",
    "\n",
    "\n",
    "        parsed_ingredient = parse_ingredient(sentences[i])\n",
    "        \n",
    "        ## filling names\n",
    "        if 'name' in parsed_ingredient[0].keys():\n",
    "\n",
    "            tmp = parsed_ingredient[0]['name']\n",
    "            useless_quantifiers = ['oz', 'fl', 'ounce']\n",
    "\n",
    "            try:\n",
    "                names = re.search(\"[^\\d]*$\", tmp).group(0)\n",
    "            except:\n",
    "                names = tmp\n",
    "\n",
    "        else:\n",
    "            names = np.nan\n",
    "            \n",
    "        ##filling units\n",
    "        #import pdb; pdb.set_trace()\n",
    "        for unit in our_units:\n",
    "            if unit in parsed_ingredient[0]['input']:\n",
    "                units[names] = unit\n",
    "            elif 'unit' in parsed_ingredient[0].keys():\n",
    "                units[names] = parsed_ingredient[0]['unit']\n",
    "        try: \n",
    "            units[names]\n",
    "        except:\n",
    "            units[names] = 'unit'\n",
    "\n",
    "        ##filling quantities\n",
    "        if re.search(\"\\dg\", sentences[i]) is not None:\n",
    "            try:\n",
    "                qtys[names] = re.search(\"\\d+(?=\\s*g)\", parsed_ingredient[0]['input']).group(0)\n",
    "            except:\n",
    "                pass\n",
    "        elif re.search(\"\\dkg\", sentences[i]) is not None:\n",
    "            try:\n",
    "                qtys[names] = re.search(\"\\d+(?=\\s*kg)\", parsed_ingredient[0]['input']).group(0)\n",
    "            except:\n",
    "                pass\n",
    "        elif 'qty' in parsed_ingredient[0].keys():\n",
    "            qtys[names] = parsed_ingredient[0]['qty']\n",
    "        else:\n",
    "            try:\n",
    "                qtys[names] = float(parsed_ingredient[0]['input'][:3])\n",
    "            except:\n",
    "                qtys[names] = np.nan\n",
    "    \n",
    "    \n",
    "    ## collating all elements together\n",
    "    names_list = []\n",
    "    units_list = []\n",
    "    qtys_list = []\n",
    "    for key in units.keys():\n",
    "        names_list.append(key)\n",
    "        qtys_list.append(qtys[key])\n",
    "        units_list.append(units[key])\n",
    "        \n",
    "    final_df = pd.DataFrame(list(zip(qtys_list, units_list, names_list)), columns = ['qty', 'unit', 'name'])\n",
    "\n",
    "    final_df = final_df[final_df['name'] != '']\n",
    "    final_df = final_df[final_df['name'].notna()]\n",
    "    final_df = final_df[final_df['unit'].notna()]\n",
    "\n",
    "    final_df.loc[final_df['unit'] == 'teaspoon', 'qty'] = 0\n",
    "    final_df.loc[final_df['unit'] == 'teaspoon', 'unit'] = 'gram'\n",
    "    final_df.loc[final_df['qty'].astype(str) == 'nan', 'qty'] = 1\n",
    "\n",
    "    final_df.reset_index()\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_list,b,c = get_ingredients_url('https://www.bbc.co.uk/food/recipes/stuffedchickenbreast_89240')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = parse_recipe_ingredients(ingredient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c3fbd2e42269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munits_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mqtys_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnames_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mqtys_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'units' is not defined"
     ]
    }
   ],
   "source": [
    "names_list = []\n",
    "units_list = []\n",
    "qtys_list = []\n",
    "for key in units.keys():\n",
    "    names_list.append(key)\n",
    "    qtys_list.append(qtys[key])\n",
    "    units_list.append(units[key])\n",
    "\n",
    "test_df = pd.DataFrame(list(zip(qtys_list, units_list, names_list)), columns = ['qty', 'unit', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breast',\n",
       " 'thighs',\n",
       " 'unit',\n",
       " 'unit',\n",
       " 'unit',\n",
       " 'tablespoon',\n",
       " 'tablespoon',\n",
       " 'gram',\n",
       " 'unit',\n",
       " 'gram',\n",
       " 'unit',\n",
       " 'unit',\n",
       " 'tablespoon',\n",
       " 'gram',\n",
       " 'tablespoon']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving calculator with new units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/Final_conversion_table.csv')\n",
    "data = data.drop(['Average tablespoon (grams)', 'Source', 'Average Unit (if applicable) (grams)', 'Source.1'], axis = 1)\n",
    "data.columns = ['name','tablespoon','unit','wing','breast','drumstick','thigh','chop','fillet','ghg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tablespoon</th>\n",
       "      <th>unit</th>\n",
       "      <th>wing</th>\n",
       "      <th>breast</th>\n",
       "      <th>drumstick</th>\n",
       "      <th>thigh</th>\n",
       "      <th>chop</th>\n",
       "      <th>fillet</th>\n",
       "      <th>ghg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcohol</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Almonds</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anise</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apples</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apricot</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Wheat &amp; Rye (Bread)</td>\n",
       "      <td>0.00981</td>\n",
       "      <td>0.053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>White bean</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Wine</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Wine vinegar</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Yogurts</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  tablespoon   unit  wing  breast  drumstick  thigh  \\\n",
       "0                Alcohol     0.01000    NaN   NaN     NaN        NaN    NaN   \n",
       "1                Almonds     0.01000    NaN   NaN     NaN        NaN    NaN   \n",
       "2                  Anise     0.01000    NaN   NaN     NaN        NaN    NaN   \n",
       "3                 Apples         NaN  0.080   NaN     NaN        NaN    NaN   \n",
       "4                Apricot     0.01000  0.060   NaN     NaN        NaN    NaN   \n",
       "..                   ...         ...    ...   ...     ...        ...    ...   \n",
       "128  Wheat & Rye (Bread)     0.00981  0.053   NaN     NaN        NaN    NaN   \n",
       "129           White bean     0.01000    NaN   NaN     NaN        NaN    NaN   \n",
       "130                 Wine     0.01475    NaN   NaN     NaN        NaN    NaN   \n",
       "131         Wine vinegar     0.01000    NaN   NaN     NaN        NaN    NaN   \n",
       "132              Yogurts     0.01000  0.080   NaN     NaN        NaN    NaN   \n",
       "\n",
       "     chop  fillet     ghg  \n",
       "0     NaN     NaN  1.5000  \n",
       "1     NaN     NaN  4.4100  \n",
       "2     NaN     NaN  2.7300  \n",
       "3     NaN     NaN  0.4000  \n",
       "4     NaN     NaN  0.6315  \n",
       "..    ...     ...     ...  \n",
       "128   NaN     NaN  1.3000  \n",
       "129   NaN     NaN  1.7850  \n",
       "130   NaN     NaN  1.6000  \n",
       "131   NaN     NaN  4.1700  \n",
       "132   NaN     NaN  2.5240  \n",
       "\n",
       "[133 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== printing directory =======\n",
      "===== printing content directory =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgesdelrieu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/georgesdelrieu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/georgesdelrieu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/georgesdelrieu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from main_calculation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matching import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matching_objects2.pk', 'rb') as handle:\n",
    "    matching_objects_dict = cPickle.load(handle)\n",
    "    vectorizer = matching_objects_dict['vectorizer']\n",
    "    df_wiki_match_scores = matching_objects_dict['df_wiki_match_scores']\n",
    "    category_list = matching_objects_dict['category_list']\n",
    "    category_summary_vectors = matching_objects_dict[\n",
    "        'category_summary_vectors']\n",
    "    category_summaries = matching_objects_dict['category_summaries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<132x3970 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13502 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(category_summaries[:132])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_match(ingredient):\n",
    "\n",
    "    try:\n",
    "        ingredient, url, url_base = get_google_cse_result(ingredient)\n",
    "    except:\n",
    "        return 'nomatch', 0\n",
    "\n",
    "    pageid = get_pageid_from_base(url_base)\n",
    "\n",
    "    pagesummary = get_summary_from_id(pageid)\n",
    "\n",
    "    processed_summary = pre_process_summary(pagesummary)\n",
    "\n",
    "    summary_vector = vectorizer.transform([processed_summary])\n",
    "    \n",
    "    return summary_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3970 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 34 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_google_match('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_and_score(summary_vector):\n",
    "    category_summary_vectors = vectorizer.transform(category_summaries)\n",
    "    scoreseries = []\n",
    "\n",
    "    for j, _ in enumerate(category_summaries):\n",
    "        cosine_sum = 1 - spatial.distance.cosine(\n",
    "            summary_vector.toarray(), category_summary_vectors[j, :].toarray())\n",
    "\n",
    "        scoreseries.append(cosine_sum)\n",
    "\n",
    "    matchscore = max(scoreseries)\n",
    "    match = category_list[scoreseries.index(matchscore)]\n",
    "\n",
    "    return match, matchscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<132x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13502 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_summary_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n",
      "(1, 3970)\n",
      "(133, 3970)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Pork Meat', 0.21647900132201947)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_match_and_score(get_google_match('ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pork Meat', 0.21647900132201947)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_match_and_score(get_google_match('ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
